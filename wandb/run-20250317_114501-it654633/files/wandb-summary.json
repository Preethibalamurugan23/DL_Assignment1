{"Loss & Accuracy Comparison":{"sha256":"7df0e9ae2dbf3bd55e42361d4c614e9eb3e47c541f062a02119fa7c672eb880e","size":106333,"path":"media/images/Loss & Accuracy Comparison_1_7df0e9ae2dbf3bd55e42.png","format":"png","width":1200,"height":1000,"_type":"image-file"},"_timestamp":1.742204253990332e+09,"_runtime":135.3821812,"_step":3,"test_accuracy":0.8706,"test_loss":0.37070756031078483,"confusion_matrix_table":{"path":"media/table/confusion_matrix_table_3_345974c248ca5cf234a9.table.json","ncols":3,"nrows":100,"_type":"table-file","sha256":"345974c248ca5cf234a9c04c0dd958dd03750796f407417fd0c589248a940ea3","size":1814,"artifact_path":"wandb-client-artifact://m2g3kaz5rqabdeoppykzf25jzzcsrrsq4538up7vfm3vxscn5jct57pp8uxaz6iumjrkgolm4kgri447btvqf9k90g8bt0mq21yv5e3vtmodv88ctxno6j9dgd90qbbe/confusion_matrix_table.table.json","_latest_artifact_path":"wandb-client-artifact://dw1b91nel7cz5fhkdesvalpsuiujzr4vpen8ia9n7v127styynehz5jmbknf6whxt3ask6z3qtgzyt2ejbpzvm3u930oy26ocsd3ny3fl48e2kml9smbtgm5jb8rcdzm:latest/confusion_matrix_table.table.json"},"_wandb":{"runtime":135}}